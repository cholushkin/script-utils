// --- Source Blob ---

// --- Start File: ScriptUtils\Core\BackupUtil\BackupUtil.py ---

import os
import zipfile
import time
from datetime import datetime
from fnmatch import fnmatch
import sys

# Add Core to path so we can import ConfigManager, LogManager
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
from Core.ConfigManager import ConfigManager
from Core.LogManager import LogManager

class BackupUtil:
    def __init__(self, destination_path, folders, excludes=None):
        # Correcting the project root
        self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
        self.destination_path = destination_path
        self.folders = [os.path.join(self.project_root, folder) for folder in folders]  # Correct the folder paths
        self.excludes = excludes or []

        # Replace {DATE} in destination path
        current_date = datetime.now().strftime("%Y-%m-%d")
        self.destination_zip = os.path.abspath(destination_path.replace("{DATE}", current_date))
        self.compression_level = zipfile.ZIP_DEFLATED

        # Load config
        self.config_manager = ConfigManager()
        self.config = self.config_manager.load_config()
        self.log_level = self.config.get("log_level", "balanced")
        self.log_manager = LogManager(self.log_level) # Initialize LogManager

        self.log_manager.log("verbose", "=== BackupUtil Initialized ===")
        self.log_manager.log("verbose", "Project root: " + self.project_root)
        self.log_manager.log("verbose", "Folders to backup:")
        for folder in self.folders:
            self.log_manager.log("verbose", "  -> " + folder)
        self.log_manager.log("verbose", "Exclude patterns: " + str(self.excludes))
        self.log_manager.log("verbose", "Destination zip path: " + self.destination_zip)
        self.log_manager.log("verbose", "Compression level: ZIP_DEFLATED")

    def is_excluded(self, filepath):
        rel_path = os.path.relpath(filepath, self.project_root)
        result = any(fnmatch(rel_path, pattern) for pattern in self.excludes)
        if result:
            self.log_manager.log("verbose", "Excluded by pattern: " + rel_path)
        return result

    def get_next_filename(self, filename):
        base, ext = os.path.splitext(filename)
        counter = 2
        new_filename = f"{base} ({counter}){ext}"
        while os.path.exists(new_filename):
            counter += 1
            new_filename = f"{base} ({counter}){ext}"
        self.log_manager.log("normal", "Existing backup found. New name: " + new_filename)
        return new_filename

    def backup(self):
        start_time = time.time()
        self.log_manager.log("normal", "Starting backup...")

        # Check if the folders exist
        for folder in self.folders:
            if not os.path.exists(folder):
                self.log_manager.log("important", "Error: Folder not found - " + folder)

        if os.path.exists(self.destination_zip):
            self.log_manager.log("normal", self.destination_zip + " already exists.")
            self.destination_zip = self.get_next_filename(self.destination_zip)

        os.makedirs(os.path.dirname(self.destination_zip), exist_ok=True)
        self.log_manager.log("normal", "Creating archive: " + self.destination_zip)

        total_files = 0
        with zipfile.ZipFile(self.destination_zip, 'w', self.compression_level) as zipf:
            for folder in self.folders:
                if not os.path.exists(folder):
                    self.log_manager.log("important", "Warning: Folder not found - " + folder)
                    continue
                for root, dirs, files in os.walk(folder):
                    self.log_manager.log("verbose", "Scanning directory: " + root)
                    for file in files:
                        full_path = os.path.join(root, file)
                        if not os.path.exists(full_path):
                            self.log_manager.log("verbose", "Skipped non-existent: " + full_path)
                            continue
                        if self.is_excluded(full_path):
                            continue
                        arcname = os.path.relpath(full_path, self.project_root)
                        zipf.write(full_path, arcname)
                        total_files += 1
                        self.log_manager.log("verbose", "Added: " + arcname)

        duration = time.time() - start_time
        self.log_manager.log("normal", "Backup complete: " + self.destination_zip)
        self.log_manager.log("normal", "Files added: " + str(total_files))
        self.log_manager.log("normal", "Duration: " + str(duration) + " seconds")

    def run(self):
        self.backup()

// --- End File: ScriptUtils\Core\BackupUtil\BackupUtil.py ---

// --- Start File: ScriptUtils\Config\default_config.json ---

{
    "log_level": "verbose",
    "unity_project_root": "Unity/GameName"
}


// --- End File: ScriptUtils\Config\default_config.json ---

// --- Start File: ScriptUtils\UserScriptsExamples\AddNewUdioTracks.py ---

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from Core.Udio.AddNewTracks import AddNewTracks

AddNewTracks( 
    source_directory = r"UdioSources\UdioDownload", 
    track_target_path = r"UdioSources\tracks",
    move_files = False
    # log_level = "important"  - You can override global log level. Overriding chain: Config/default_config.jsong <- Config/user_config.jsong <- value from parameter log_level
).run()

// --- End File: ScriptUtils\UserScriptsExamples\AddNewUdioTracks.py ---

// --- Start File: ScriptUtils\UserScriptsExamples\BackupUdio.py ---

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from Core.BackupUtil.BackupUtil import BackupUtil

folders_to_backup = ["UdioSources", "Unity"]
exclude_patterns = ["*.tmp", "*.log", "__pycache__"]
destination_path = "d:/backups/udio-backup-{DATE}.zip"

# Run backup
BackupUtil(destination_path, folders_to_backup, exclude_patterns).run()

// --- End File: ScriptUtils\UserScriptsExamples\BackupUdio.py ---

// --- Start File: ScriptUtils\UserScriptsExamples\MakePromptAdaptScriptForScriptUtils.py ---

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from Core.PromptContextCollector.PromptContextCollector import PromptContextCollector

PromptContextCollector(
    directories=[
        "ScriptUtils/Core/**",
        "ScriptUtils/Config/*.json",
        "ScriptUtils/UserScriptsExamples/*.py"
    ],  # Wildcards and folders
    includes=["*.py","*.json"],
    ignores=["*/Tests/*", "*.meta", "__pycache__", "ScriptUtils/Core/Udio/Effects", "ScriptUtils/Core/Udio/AddNewTracksJsonTemplate.json"],
    template_path="ScriptUtils/UserScriptsExamples/MakePromptAdaptScriptForScriptUtilsTemplate.txt",  # Relative to project root
    template_vars={"target_script": "git_submodule_manager"},
    output_path="ScriptUtils/UserScriptsExamples/MakePromptOutput/PromptAdaptScriptForScriptUtils.txt"
).run()

// --- End File: ScriptUtils\UserScriptsExamples\MakePromptAdaptScriptForScriptUtils.py ---

// --- Start File: ScriptUtils\UserScriptsExamples\MakePromptAnalyzeScript.py ---

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from Core.PromptContextCollector.PromptContextCollector import PromptContextCollector

PromptContextCollector(
    directories=["ScriptUtils/Core/PromptContextCollector/**"],  # Wildcards and folders
    includes=["*.py"],
    ignores=["*/Tests/*", "*.meta", "__pycache__"],
    template_path="ScriptUtils/UserScriptsExamples/MakePromptAnalyzeScriptTemplate.txt",  # Relative to project root
    template_vars={"additional_task_0": "Share ideas to expand the concept and introduce innovative or interesting new features."},
    output_path="ScriptUtils/UserScriptsExamples/MakePromptOutput/PromptContextCollectorScriptAnalysis.txt"
).run()

// --- End File: ScriptUtils\UserScriptsExamples\MakePromptAnalyzeScript.py ---

// --- Start File: ScriptUtils\UserScriptsExamples\MakePromptMakeReadme.py ---

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from Core.PromptContextCollector.PromptContextCollector import PromptContextCollector

PromptContextCollector(
    directories=["ScriptUtils/Core/PromptContextCollector/**"],  # Wildcards and folders
    includes=["*.py"],
    ignores=["*/Tests/*", "*.meta", "__pycache__"],
    template_path="ScriptUtils/UserScriptsExamples/MakePromptMakeReadmeTemplate.txt",  # Relative to project root
    template_vars={"additional_task_0": "Provide two versions of your response: one concise and one more detailed."},
    output_path="ScriptUtils/UserScriptsExamples/MakePromptOutput/PromptMakeReadmeForPromptCollector.txt"
).run()

// --- End File: ScriptUtils\UserScriptsExamples\MakePromptMakeReadme.py ---

// --- Start File: ScriptUtils\UserScriptsExamples\PrintConfig.py ---

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "Core")))
from ConfigManager import ConfigManager

ConfigManager().print()

// --- End File: ScriptUtils\UserScriptsExamples\PrintConfig.py ---

// --- Start File: ScriptUtils\UserScriptsExamples\UdioAnalyzeTracks.py ---

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from Core.Udio.AnalyzeTrackDB  import AnalyzeTrackDB 

AnalyzeTrackDB(
    look_folders=["d:/projects/dev/target-one/SoundSources/udio/tracks"],
    estimate_bitrates=["48k", "128k", "192k"],
).run()

// --- End File: ScriptUtils\UserScriptsExamples\UdioAnalyzeTracks.py ---

// --- Start File: ScriptUtils\UserScriptsExamples\UdioExportToUnity.py ---

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from Core.Udio.ExportTracks import ExportTracks

ExportTracks(
    look_folders = ["UdioSources/tracks"], 
    unity_dest_path = "Assets/Music", 
).run()

// --- End File: ScriptUtils\UserScriptsExamples\UdioExportToUnity.py ---

// --- Start File: ScriptUtils\UserScriptsExamples\UdioMixTracks.py ---

import os, sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from Core.Udio.MixTracks  import MixTracks 

MixTracks(
    look_folders=["UdioSources/tracks"],
    mix_override={
        "taraget_wildcard" : ["Ambient Echoes 15", "Hypnotic*"], # all tracks starting with 'Hypnotic' and 'Ambient Echoes 15'
        "json" : { # values presented here will override the original mix part of json
            "ignore": False, 
            "bitrate": "48k", # convert all of them to 48k
            "effects": {} # effects are overriden with empty dict, that means no effects will be applied (original dry mix)
        }        
    }
).run()

// --- End File: ScriptUtils\UserScriptsExamples\UdioMixTracks.py ---

// --- Start File: ScriptUtils\Core\Udio\AddNewTracks.py ---

import os
import shutil
import re
import sys
import json
from collections import defaultdict

# Add Core to path so we can import ConfigManager, LogManager
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
from Core.ConfigManager import ConfigManager
from Core.LogManager import LogManager

class AddNewTracks:
    """
    Moves or copies MP3 and ZIP file pairs into organized subfolders, handling naming and error conditions.
    All paths are relative to the UserProject root, which is calculated automatically.
    Also creates a default JSON file for each track.  JSON template is loaded in the run method.
    """
    def __init__(self, source_directory, track_target_path, move_files=True, log_level = None):
        """
        Initializes the AddNewTrack object.

        Args:
            source_directory (str): The path to the directory containing the MP3 and ZIP files,
                                     relative to the project root.
            track_target_path (str): The path to the target directory *relative* to the UserProject root
                                     where track subfolders will be created (e.g., "tracks").
            move_files (bool, optional): If True, files are moved; if False, files are copied. Defaults to True.
        """
        # Calculate project root dynamically
        self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
        self.source_path = os.path.join(self.project_root, source_directory)  # Make source_path absolute
        self.track_target_path = os.path.join(self.project_root, track_target_path)
        self.move_files = move_files
        self.files_processed_count = 0
        self.folders_created_count = 0        
        self.action_word = "Moved" if move_files else "Copied"
        self.file_operation = shutil.move if move_files else shutil.copy2
        self.count_description = "Files moved" if move_files else "Files copied"

        # Load config
        self.config_manager = ConfigManager()
        self.config = self.config_manager.load_config()
        self.log_manager = LogManager(self.config.get("log_level", "verbose")) # Initialize LogManager
        if log_level != None:
            self.log_manager.globalLogLevel = log_level


    def ensure_target_root_exists(self):
        """Ensures that the target root directory exists."""
        if not os.path.isdir(self.track_target_path):
            try:
                os.makedirs(self.track_target_path)
                self.log_manager.log("normal", f"üìÅ Created target root directory: {self.track_target_path}")
                self.folders_created_count += 1
            except OSError as e:
                self.log_manager.log("important", f"‚ùå Error: Could not create target root directory '{self.track_target_path}'. {e}")
                return False
        return True

    def get_source_files(self):
        """
        Gets all files from the source directory and performs initial validation.
        """
        try:
            all_items = os.listdir(self.source_path)
            source_files = [f for f in all_items if os.path.isfile(os.path.join(self.source_path, f))]
        except OSError as e:
            self.log_manager.log("important", f"‚ùå Error: Could not read source directory '{self.source_path}'. {e}")
            return None, None, None, None

        mp3_files = set()
        zip_files = set()
        basenames_mp3 = set()
        basenames_zip = set()

        for f in source_files:
            name, ext = os.path.splitext(f)
            ext_lower = ext.lower()
            if ext_lower == ".mp3":
                mp3_files.add(f)
                basenames_mp3.add(name)
            elif ext_lower == ".zip":
                zip_files.add(f)
                basenames_zip.add(name)
        return mp3_files, zip_files, basenames_mp3, basenames_zip

    def validate_file_pairs(self, basenames_mp3, basenames_zip):
        """Validates that there is a matching .mp3 and .zip file for each base name."""
        inconsistent_basenames = basenames_mp3.symmetric_difference(basenames_zip)

        if inconsistent_basenames:
            self.log_manager.log("important", "‚ùå Error: File inconsistency detected. Found '.mp3' without matching '.zip' or vice-versa.")
            self.log_manager.log("important", "Inconsistent base names:")
            for base in inconsistent_basenames:
                if base in basenames_mp3:
                    self.log_manager.log("important", f"  - Found '{base}.mp3' without matching '.zip'")
                else:
                    self.log_manager.log("important", f"  - Found '{base}.zip' without matching '.mp3'")
            return None

        return basenames_mp3.intersection(basenames_zip)

    def create_track_json(self, folder_name, track_name, json_template):
        """Creates the track JSON file."""
        if json_template is None:
            return

        # Construct the JSON file path.
        json_file_name = f"{track_name}.json"
        dest_json_path = os.path.join(self.track_target_path, folder_name, json_file_name)

        # Modify the template
        updated_template = json_template.copy()
        updated_template["mix"]["source_path"] = f"{track_name}.zip"
        updated_template["mix"]["output_file"] = f"{track_name}.mp3"

        try:
            with open(dest_json_path, 'w', encoding='utf-8') as f:
                json.dump(updated_template, f, indent=4)
            self.log_manager.log("normal", f"üìù Created JSON file: {os.path.join(folder_name, json_file_name)}")
        except Exception as e:
            self.log_manager.log("important", f"‚ùå Error: Could not create JSON file '{dest_json_path}': {e}")

    def process_files(self, valid_basenames, json_template):
        """Processes the valid MP3/ZIP file pairs."""
        if not valid_basenames:
            self.log_manager.log("important", "‚ÑπÔ∏è No MP3/ZIP pairs found in the source directory.")
            return

        self.log_manager.log("normal", f"‚ÑπÔ∏è Found {len(valid_basenames)} valid MP3/ZIP pairs for processing.")

        sorted_basenames = sorted(list(valid_basenames))

        for base_name_extless in sorted_basenames:
            source_mp3 = base_name_extless + ".mp3"
            source_zip = base_name_extless + ".zip"

            source_mp3_full_path = os.path.join(self.source_path, source_mp3)
            source_zip_full_path = os.path.join(self.source_path, source_zip)

            # Double check
            if not os.path.exists(source_mp3_full_path) or not os.path.exists(source_zip_full_path):
                self.log_manager.log("important", f"‚ö†Ô∏è Warning: Source file(s) for '{base_name_extless}' disappeared before processing. Skipping.")
                continue

            # Determine subfolder name
            match = re.match(r"^(.*?)(?: \(\d+\))?$", base_name_extless)
            folder_name = match.group(1).strip() if match else base_name_extless.strip()
            dest_folder_path = os.path.join(self.track_target_path, folder_name)
            relative_dest_folder = os.path.join(folder_name)

            # Create subfolder
            if not os.path.isdir(dest_folder_path):
                try:
                    os.makedirs(dest_folder_path)
                    self.log_manager.log("verbose", f"üìÅ Subfolder created: {relative_dest_folder}")
                    self.folders_created_count += 1
                except OSError as e:
                    self.log_manager.log("important", f"‚ùå Error: Could not create subfolder '{dest_folder_path}'. Skipping '{base_name_extless}'. {e}")
                    continue

            # Determine next number
            next_number = 1
            try:
                existing_files = os.listdir(dest_folder_path)
                max_num = 0
                pattern = re.compile(r"^" + re.escape(folder_name) + r" (\d+)\.(mp3|zip)$", re.IGNORECASE)
                for fname in existing_files:
                    file_match = pattern.match(fname)
                    if file_match:
                        num = int(file_match.group(1))
                        if num > max_num:
                            max_num = num
                next_number = max_num + 1
            except OSError as e:
                self.log_manager.log("important", f"‚ùå Error: Could not read destination subfolder '{dest_folder_path}'. Skipping '{base_name_extless}'. {e}")
                continue

            # Construct new paths
            new_mp3_name = f"{folder_name} {next_number}.mp3"
            new_zip_name = f"{folder_name} {next_number}.zip"
            dest_mp3_path = os.path.join(dest_folder_path, new_mp3_name)
            dest_zip_path = os.path.join(dest_folder_path, new_zip_name)

            # Process files
            try:
                # --- Process MP3 file ---
                self.file_operation(source_mp3_full_path, dest_mp3_path)
                log_dest_mp3 = os.path.join(relative_dest_folder, new_mp3_name)
                self.log_manager.log("normal", f"‚û°Ô∏è {self.action_word} {source_mp3} to {log_dest_mp3}")
                self.files_processed_count += 1

                # Create JSON file
                track_name = f"{folder_name} {next_number}"  # Base name for the track
                self.create_track_json(folder_name, track_name, json_template)


                try:
                    # --- Process ZIP file ---
                    if not os.path.exists(source_zip_full_path):
                        self.log_manager.log("important", f"‚ùå Error: Source ZIP '{source_zip}' disappeared before it could be {self.action_word.lower()}. MP3 was already {self.action_word.lower()}.")
                        if not self.move_files and os.path.exists(dest_mp3_path):
                            try:
                                os.remove(dest_mp3_path)
                                self.log_manager.log("important", f"üßπ Cleaned up partially {self.action_word.lower()} file: {log_dest_mp3}")
                                self.files_processed_count -= 1
                            except OSError as e_rem:
                                self.log_manager.log("important", f"‚ö†Ô∏è Warning: Could not clean up partially {self.action_word.lower()} MP3 '{log_dest_mp3}'. {e_rem}")
                    else:
                        self.file_operation(source_zip_full_path, dest_zip_path)
                        log_dest_zip = os.path.join(relative_dest_folder, new_zip_name)
                        self.log_manager.log("normal", f"‚û°Ô∏è {self.action_word} {source_zip} to {log_dest_zip}")
                        self.files_processed_count += 1

                except Exception as e_zip:
                    self.log_manager.log("important", f"‚ùå Error: Failed to {self.action_word.lower()} ZIP file '{source_zip}' after {self.action_word.lower()}ing MP3. {e_zip}")
                    if not self.move_files and os.path.exists(dest_mp3_path):
                        try:
                            os.remove(dest_mp3_path)
                            self.log_manager.log("important", f"üßπ Cleaned up partially {self.action_word.lower()} file: {log_dest_mp3}")
                            self.files_processed_count -= 1
                        except OSError as e_rem:
                            self.log_manager.log("important", f"‚ö†Ô∏è Warning: Could not clean up partially {self.action_word.lower()} MP3 '{log_dest_mp3}'. {e_rem}")

            except Exception as e_mp3:
                self.log_manager.log("important", f"‚ùå Error: Failed to {self.action_word.lower()} MP3 file '{source_mp3}'. Skipping pair. {e_mp3}")

    def run(self):
        """Runs the AddNewTrack process."""
        self.log_manager.log("normal", f"‚ÑπÔ∏è Source directory: {self.source_path}")
        self.log_manager.log("normal", f"‚ÑπÔ∏è Target root directory: {self.track_target_path}")
        self.log_manager.log("normal", f"‚ÑπÔ∏è Mode: {'Move files' if self.move_files else 'Copy files'}")
        self.log_manager.log("normal", "-" * 30)

        if not self.ensure_target_root_exists():
            sys.exit(1)

        # Load JSON template
        json_template_file = "AddNewTracksJsonTemplate.json"  # Default template name
        template_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), json_template_file) # Gets the directory of the current script
        if not os.path.exists(template_path):
            self.log_manager.log("important", f"‚ùå Error: JSON template file not found at '{template_path}'.")
            json_template = None  # Set to None to prevent errors later
        else:
            try:
                with open(template_path, 'r', encoding='utf-8') as f:
                    json_template = json.load(f)
            except json.JSONDecodeError as e:
                self.log_manager.log("important", f"‚ùå Error: Invalid JSON in template file '{template_path}': {e}")
                json_template = None

        mp3_files, zip_files, basenames_mp3, basenames_zip = self.get_source_files()
        if (mp3_files, zip_files, basenames_mp3, basenames_zip) == (None, None, None, None):
            sys.exit(1)

        valid_basenames = self.validate_file_pairs(basenames_mp3, basenames_zip)
        if valid_basenames is None:
            sys.exit(1)

        self.process_files(valid_basenames, json_template)

        self.log_manager.log("normal", "-" * 30)
        self.log_manager.log("normal", "‚úÖ Processing complete.")
        self.log_manager.log("normal", "üìä Summary:")
        self.log_manager.log("important", f"  ‚úÖ {self.count_description}: {self.files_processed_count}")
        self.log_manager.log("normal", f"  ‚úÖ Subfolders created: {self.folders_created_count}")

// --- End File: ScriptUtils\Core\Udio\AddNewTracks.py ---

// --- Start File: ScriptUtils\Core\Udio\AnalyzeTrackDB.py ---

import os
import sys
import io
import json
import hashlib
import time
from mutagen.mp3 import MP3

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

class AnalyzeTrackDB:
    def __init__(self, look_folders, estimate_bitrates):
        self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))  # project root
        self.look_folders = [os.path.join(self.project_root, folder) for folder in look_folders]
        self.estimate_bitrates = estimate_bitrates
        self.track_data = []

    def log(self, message):
        print(message)  # Output directly to console

    def get_mp3_duration(self, file_path):
        try:
            audio = MP3(file_path)
            duration = int(audio.info.length)
            minutes = duration // 60
            seconds = duration % 60
            return duration, f"{minutes}:{seconds:02d}"
        except Exception:
            return 0, "Unknown"

    def read_track_metadata_from_json(self, json_file):
        metadata = {"energy": None, "mood": None, "pop": None, "stars": None}
        if os.path.exists(json_file):
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    if "tags" in data:
                        metadata.update({
                            "energy": data["tags"].get("energy"),
                            "mood": data["tags"].get("mood"),
                            "pop": data["tags"].get("pop"),
                            "stars": data["tags"].get("stars")
                        })
            except Exception as e:
                self.log(f"‚ö†Ô∏è Error reading {json_file}: {e}")
        return metadata

    def estimate_size(self, total_duration):
        sizes = {}
        for bitrate in self.estimate_bitrates:
            kbps = int(bitrate.replace("k", ""))
            mb = (total_duration * kbps / 8) / 1024
            sizes[bitrate] = mb
        return sizes

    def run(self):
        start_time = time.time()

        total_size = 0
        total_duration = 0
        mp3_count = 0
        md5_hashes = {}
        name_duplicates = {}

        self.log("üîç Searching for MP3 files with corresponding JSON metadata...\n")

        for folder in self.look_folders:
            if not os.path.isdir(folder):
                self.log(f"‚ö†Ô∏è Folder not found: {folder}")
                continue

            self.log(f"üìÇ Searching in: {folder}")
            for root, _, files in os.walk(folder):
                for file in files:
                    if not file.lower().endswith(".mp3"):
                        continue

                    file_path = os.path.join(root, file)
                    json_file = os.path.splitext(file_path)[0] + ".json"
                    file_name = os.path.splitext(file)[0]
                    has_json = os.path.exists(json_file)
                    json_emoji = " üìÑ" if has_json else " üìÑ‚ùå"

                    if not has_json:
                        self.log(f"‚ö†Ô∏è Warning: JSON metadata not found for {file}")
                        continue

                    metadata = self.read_track_metadata_from_json(json_file)
                    energy = metadata["energy"]
                    mood = metadata["mood"]
                    pop = metadata["pop"]
                    stars = metadata["stars"]

                    duration_seconds, duration_str = self.get_mp3_duration(file_path)

                    md5 = hashlib.md5(open(file_path, 'rb').read()).hexdigest()
                    md5_hashes.setdefault(md5, []).append(file_path)
                    name_duplicates.setdefault(file, []).append(file_path)

                    # Format and log metadata
                    line = f"- {file} [{duration_str}]{json_emoji}"
                    if energy is not None: line += f" üî•{energy:.1f}"
                    if mood is not None:    line += f" üòä{mood:.1f}"
                    if pop is not None:     line += f" üéµ{pop:.1f}"
                    if stars is not None:   line += f" ‚ú®{int(stars)}"

                    self.log(line)

                    self.track_data.append({
                        "name": file_name,
                        "stars": int(stars) if stars is not None else None,
                        "mood": mood,
                        "energy": energy,
                        "pop": pop,
                    })

                    total_size += os.path.getsize(file_path)
                    total_duration += duration_seconds
                    mp3_count += 1

        self.log("\n=== Summary ===")
        self.log(f"üéº Total MP3 files analyzed (with JSON metadata): {mp3_count}")
        self.log(f"üíæ Total Space Occupied: {total_size / (1024 * 1024):.2f} MB")
        self.log(f"‚è±Ô∏è Total Duration: {total_duration // 3600:02d}:{(total_duration % 3600) // 60:02d}:{total_duration % 60:02d}")

        self.log("\nüìè Estimated Total Size at Different Bitrates:")
        for bitrate, size in self.estimate_size(total_duration).items():
            self.log(f"üì° {bitrate}: {size:.2f} MB")

        md5_duplicates = [v for v in md5_hashes.values() if len(v) > 1]
        name_duplicates_filtered = [v for v in name_duplicates.values() if len(v) > 1]

        if md5_duplicates or name_duplicates_filtered:
            self.log("\nüìÅ **Duplicate Files Found:**")
            count = 1
            for group in md5_duplicates:
                for i in range(len(group) - 1):
                    self.log(f"{count}. \"{group[i]}\" and \"{group[i+1]}\" (Same MD5)")
                    count += 1
            for group in name_duplicates_filtered:
                for i in range(len(group) - 1):
                    self.log(f"{count}. \"{group[i]}\" and \"{group[i+1]}\" (Same Name)")
                    count += 1

        elapsed = time.time() - start_time
        self.log(f"\nüïí Execution Time: {int(elapsed // 3600):02d}:{int((elapsed % 3600) // 60):02d}:{int(elapsed % 60):02d}")
        self.log("\n‚úÖ Finished analyzing files.")


// --- End File: ScriptUtils\Core\Udio\AnalyzeTrackDB.py ---

// --- Start File: ScriptUtils\Core\Udio\ExportTracks.py ---

import os
import shutil
import json
import sys

# Add Core to path so we can import ConfigManager, LogManager
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
from Core.ConfigManager import ConfigManager
from Core.LogManager import LogManager


class ExportTracks:
    def __init__(self, look_folders, unity_dest_path, global_log_level=None):
        """
        Initializes the ExportTracks class.

        Args:
            look_folders (list): A list of folders to search for track files.
            unity_dest_path (str): The destination path within the Unity project
                                    to copy the tracks to.
            global_log_level (str, optional): The global log level
                                    ("important", "normal", "verbose", "disabled").
                                    If None, it defaults to the value in the config file.
        """
        self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
        self.look_folders = [os.path.join(self.project_root, folder) for folder in look_folders]

        # Initialize ConfigManager
        self.config_manager = ConfigManager()
        self.config = self.config_manager.load_config()

        # Initialize LogManager
        self.log_manager = LogManager(self.config.get("log_level", "verbose"))
        if global_log_level is not None:
            self.log_manager.globalLogLevel = global_log_level

        self.unity_project_root = self.config.get("unity_project_root")
        if not self.unity_project_root:
            self.log_manager.log(
                "important",
                "‚ùå Unity project root is not set in the configuration.  Please set the 'unity_project_root' in config.json.",
            )
            sys.exit(1)  # Exit if the Unity project root is not configured
            
        self.unity_project_root = os.path.join(self.project_root, self.unity_project_root)            
        self.unity_dest_path = os.path.join(self.unity_project_root, unity_dest_path)
        os.makedirs(self.unity_dest_path, exist_ok=True)  # Ensure destination directory exists

    def _find_track_files(self):
        """Finds all track JSON files within the look_folders."""
        track_files = []
        for folder in self.look_folders:
            if not os.path.isdir(folder):
                self.log_manager.log("important", f"‚ùå Folder not found: {folder}")
                continue
            self.log_manager.log("verbose", f"üìÇ Searching for tracks in: {folder}")
            for root, _, files in os.walk(folder):
                for file in files:
                    if file.lower().endswith(".json"):
                        full_path = os.path.join(root, file)
                        track_files.append(full_path)
                        self.log_manager.log("verbose", f"    ‚úÖ Found track JSON: {full_path}")
        return track_files

    def _process_track(self, track_json_path):
        """
        Processes a single track JSON file, copying the MP3 and creating metadata.

        Args:
            track_json_path (str): Path to the track JSON file.
        """
        self.log_manager.log("normal", f"\n‚ñ∂Ô∏è Processing track: {track_json_path}")
        try:
            with open(track_json_path, "r", encoding="utf-8") as f:
                track_data = json.load(f)
        except FileNotFoundError:
            self.log_manager.log("important", f"‚ùå JSON file not found: {track_json_path}. Skipping.")
            return
        except json.JSONDecodeError as e:
            self.log_manager.log("important", f"‚ùå Error decoding JSON: {track_json_path} - {e}. Skipping.")
            return
        except Exception as e:
            self.log_manager.log("important", f"‚ùå Error reading JSON file: {track_json_path} - {e}. Skipping.")
            return

        # --- Check for 'mix' section and 'export' parameter ---
        if "mix" not in track_data:
            self.log_manager.log("important", f"‚ùå 'mix' section not found in JSON: {track_json_path}. Skipping.")
            return
        mix_config = track_data["mix"]

        export_params = track_data.get("export_parameters", {})
        if export_params.get("export", True) is False:  # Default to True if not present
            self.log_manager.log("normal", f"‚è≠Ô∏è Track not marked for export in JSON: {track_json_path}")
            return

        # --- Get MP3 source path ---
        source_mp3_name = mix_config.get("output_file")
        if not source_mp3_name:
            self.log_manager.log(
                "important", f"‚ùå 'output_file' not found in 'mix' section of JSON: {track_json_path}. Skipping."
            )
            return

        source_mp3_path = os.path.join(os.path.dirname(track_json_path), source_mp3_name)
        if not os.path.exists(source_mp3_path):
            self.log_manager.log("important", f"‚ùå MP3 file not found: {source_mp3_path}. Skipping.")
            return

        # --- Construct destination path and copy ---
        dest_mp3_path = os.path.join(self.unity_dest_path, source_mp3_name)
        try:
            if os.path.exists(dest_mp3_path):
                self.log_manager.log("normal", f"‚ö†Ô∏è  Overwriting existing file: {dest_mp3_path}")
            shutil.copy2(source_mp3_path, dest_mp3_path)  # copy2 preserves metadata
            self.log_manager.log("verbose", f"    ‚úÖ Copied MP3 to: {dest_mp3_path}")
        except Exception as e:
            self.log_manager.log("important", f"‚ùå Error copying MP3: {source_mp3_path} to {dest_mp3_path} - {e}")
            return

        # --- Create and save metadata JSON ---
        tags = track_data.get("tags", {})
        metadata = {"tags": tags}
        dest_meta_path = os.path.join(self.unity_dest_path, os.path.splitext(source_mp3_name)[0] + ".meta.json")
        try:
            with open(dest_meta_path, "w", encoding="utf-8") as f:
                json.dump(metadata, f, indent=4)
            self.log_manager.log("verbose", f"    ‚úÖ Created metadata: {dest_meta_path}")
        except Exception as e:
            self.log_manager.log("important", f"‚ùå Error writing metadata JSON: {dest_meta_path} - e")
            return

        self.log_manager.log("normal", f"    ‚úÖ Successfully processed track: {source_mp3_name}")

    def run(self):
        """Finds and processes all track JSON files."""
        self.log_manager.log("important", "=" * 40)
        self.log_manager.log("important", "üöÄ Starting Export Tracks to Unity")
        self.log_manager.log("important", f"Project Root: {self.project_root}")
        self.log_manager.log("important", f"Searching Folders: {self.look_folders}")
        self.log_manager.log("important", f"Exporting to: {self.unity_dest_path}")
        self.log_manager.log("important", "=" * 40)

        track_json_files = self._find_track_files()
        if not track_json_files:
            self.log_manager.log("important", "‚èπÔ∏è No track JSON files found.")
            return

        self.log_manager.log("normal", f"‚ÑπÔ∏è Found {len(track_json_files)} track JSON files to process.")
        for track_json_path in track_json_files:
            self._process_track(track_json_path)
        self.log_manager.log("important", "üèÅ Export process complete.")


// --- End File: ScriptUtils\Core\Udio\ExportTracks.py ---

// --- Start File: ScriptUtils\Core\Udio\MixTracks.py ---

import os
import zipfile
import json
import shutil
import importlib.util
import tempfile
import traceback
from pydub import AudioSegment
import sys
from fnmatch import fnmatch

# Add Core to path so we can import ConfigManager, LogManager
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
from Core.ConfigManager import ConfigManager
from Core.LogManager import LogManager

# --- Script Configuration ---
EXPECTED_STEMS = ["bass.wav", "drums.wav", "other.wav", "vocals.wav"]
DEFAULT_BITRATE = "192k"

class MixTracks:
    def __init__(self, look_folders, global_log_level=None, mix_override=None):
        """
        Initializes the MixTracks processor.

        Args:
            look_folders (list[str]): List of directory paths (relative to project root)
                                        to search for track subfolders.
            global_log_level (str, optional): Desired logging level ('verbose', 'normal', 'important').
                                            Defaults to config file setting or 'verbose'.
            mix_override (dict, optional):  A dictionary to override 'mix' section values in the JSON.
        """
        self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
        self.look_folders = [os.path.join(self.project_root, folder) for folder in look_folders]
        self.mix_override = mix_override  # Store the mix override

        # Initialize ConfigManager
        self.config_manager = ConfigManager()
        self.config = self.config_manager.load_config()

        # Initialize LogManager (using the provided snippet)
        self.log_manager = LogManager(self.config.get("log_level", "verbose"))
        if global_log_level != None:
            self.log_manager.globalLogLevel = global_log_level

        # Effects path, relative to the current script's directory
        self.effects_path = os.path.join(os.path.dirname(__file__), "Effects")
        self.effects = self._load_effects()

    def _find_track_json_files(self):
        """Finds all track JSON files (e.g., 'Track Name 1.json') within the look_folders."""
        json_files = []
        for folder in self.look_folders:
            if not os.path.isdir(folder):
                self.log_manager.log("important", f"‚ùå Search folder not found: {folder}")
                continue
            self.log_manager.log("verbose", f"üìÇ Searching for tracks in: {folder}")
            for root, _, files in os.walk(folder):
                for file in files:
                    # Process any .json file, assuming it's a track config
                    if file.lower().endswith(".json"):
                        full_path = os.path.join(root, file)
                        json_files.append(full_path)
                        self.log_manager.log("verbose", f"    ‚úÖ Mapped JSON: {full_path}")
        return json_files

    def _load_effects(self):
        """Loads effect functions from Python files in the Effects directory."""
        effects = {}
        if not os.path.isdir(self.effects_path):
            self.log_manager.log("important", f"‚ùå Effects directory not found: {self.effects_path}")
            return effects

        self.log_manager.log("normal", f"üîç Loading effects from: {self.effects_path}")
        for filename in os.listdir(self.effects_path):
            if filename.lower().endswith(".py") and filename != "__init__.py":
                effect_path = os.path.join(self.effects_path, filename)
                module_name = f"effects.{os.path.splitext(filename)[0]}"  # Unique module name
                try:
                    spec = importlib.util.spec_from_file_location(module_name, effect_path)
                    if spec and spec.loader:
                        module = importlib.util.module_from_spec(spec)
                        sys.modules[module_name] = module  # Add to sys.modules before exec
                        spec.loader.exec_module(module)

                        # Corrected Effect Loading Logic
                        if hasattr(module, "effect_name") and isinstance(module.effect_name, dict):
                            for name, function in module.effect_name.items():
                                if callable(function):
                                    if name in effects:
                                        self.log_manager.log("important",
                                                            f"‚ö†Ô∏è Duplicate effect name '{name}' found in {filename}. Overwriting previous.")
                                    effects[name] = function
                                    self.log_manager.log("verbose", f"    ‚úÖ Loaded effect: '{name}' from {filename}")
                                else:
                                    self.log_manager.log("important",
                                                        f"‚ö†Ô∏è Item '{name}' in 'effect_name' from {filename} is not callable.")
                        else:
                            self.log_manager.log("important",
                                                f"‚ö†Ô∏è Effect module {filename} missing 'effect_name' dictionary or it's not a dictionary.")
                    else:
                        self.log_manager.log("important", f"‚ö†Ô∏è Could not create module spec for {filename}")

                except Exception as e:
                    self.log_manager.log("important", f"‚ùå Error loading effect module {filename}: {e}\n{traceback.format_exc()}")
        self.log_manager.log("normal", f"‚ú® Loaded {len(effects)} effects: {', '.join(effects.keys())}")
        return effects

    def process_track(self, track_json_path):
        """
        Processes a single track based on its JSON configuration file.
        Returns True on success, False on failure/skip for this track.
        """
        self.log_manager.log("normal", f"\n‚ñ∂Ô∏è Processing track from JSON: {track_json_path}")
        config_file_dir = os.path.dirname(track_json_path)

        try:
            with open(track_json_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
        except FileNotFoundError:
            self.log_manager.log("important", f"‚ùå JSON file not found: {track_json_path}")
            return False
        except json.JSONDecodeError as e:
            self.log_manager.log("important", f"‚ùå Error decoding JSON in: {track_json_path} - {e}")
            return False
        except Exception as e:
            self.log_manager.log("important", f"‚ùå Error reading JSON file {track_json_path}: {e}")
            return False

        # --- Get Mix Configuration ---
        track_config = config_data.get("mix")
        if not track_config or not isinstance(track_config, dict):
            self.log_manager.log("important",
                            f"‚ö†Ô∏è No 'mix' section found or it's not a dictionary in {track_json_path}. Skipping.")
            return False  # Treat as skippable failure

        # --- Apply Overrides ---
        if self.mix_override:
            # Check for target_wildcard
            target_wildcards = self.mix_override.get("target_wildcard", [])
            if target_wildcards:
                base_name = os.path.basename(track_json_path)
                for wildcard in target_wildcards:
                    if fnmatch(base_name, wildcard):
                        self.log_manager.log("verbose", f"   ‚úÖ Applying override for target_wildcard: {wildcard} on {base_name}")
                        track_config.update(self.mix_override.get("json", {}))  # Override the mix section
                        break  # Apply only once if matched
            else: # apply if no wildcards
                track_config.update(self.mix_override.get("json", {}))  # Override the mix section


        # --- Check Ignore Flag ---
        if track_config.get("ignore", False):
            output_name = track_config.get('output_file', os.path.basename(track_json_path).replace('.json', '.mp3'))
            self.log_manager.log("normal", f"‚è≠Ô∏è Skipping track {output_name} (ignore flag set in JSON)")
            return False  # Skipped, not a processing error

        # --- Get Paths and Parameters ---
        source_zip_name = track_config.get("source_path")
        output_mp3_name = track_config.get("output_file")
        bitrate = track_config.get("bitrate", DEFAULT_BITRATE)

        if not source_zip_name or not output_mp3_name:
            self.log_manager.log("important",
                            f"‚ùå Missing 'source_path' or 'output_file' in 'mix' section of {track_json_path}. Skipping.")
            return False

        source_zip_path = os.path.join(config_file_dir, source_zip_name)
        output_mp3_path = os.path.join(config_file_dir, output_mp3_name)

        if not os.path.exists(source_zip_path):
            self.log_manager.log("important", f"‚ùå Source ZIP file not found: {source_zip_path}")
            return False

        # --- Process Track ---
        with tempfile.TemporaryDirectory(prefix="mix_") as temp_folder:
            self.log_manager.log("verbose", f"    üì¶ Extracting {source_zip_name} to {temp_folder}")

            # Extract ZIP file
            try:
                with zipfile.ZipFile(source_zip_path, 'r') as zip_ref:
                    zip_ref.extractall(temp_folder)
            except zipfile.BadZipFile:
                self.log_manager.log("important", f"‚ùå Invalid ZIP file: {source_zip_path}")
                return False
            except Exception as e:
                self.log_manager.log("important", f"‚ùå Error extracting ZIP file {source_zip_path}: {e}")
                return False

            # Load expected audio stems and check for missing ones
            track_data = {}
            loaded_stems = []
            missing_stems = []
            for stem_file in EXPECTED_STEMS:
                file_path = os.path.join(temp_folder, stem_file)
                if os.path.exists(file_path):
                    try:
                        track_data[stem_file] = AudioSegment.from_wav(file_path)
                        loaded_stems.append(stem_file)
                        self.log_manager.log("verbose", f"    üîä Loaded stem: {stem_file}")
                    except Exception as e:
                        # Treat loading error as a missing stem for simplicity
                        self.log_manager.log("important",
                                            f"‚ùå Error loading stem {stem_file} from {source_zip_path}: {e}")
                        missing_stems.append(f"{stem_file} (Load Error)")
                else:
                    missing_stems.append(stem_file)

            # --- Strict Stem Check ---
            if missing_stems:
                self.log_manager.log("important",
                                    f"‚ùå Missing required stems in {source_zip_name}: {', '.join(missing_stems)}. Skipping mix.")
                return False  # Quit processing this track due to missing stems

            self.log_manager.log("verbose", f"    üîä All expected stems loaded: {', '.join(loaded_stems)}")

            # Apply effects dynamically
            effects_config = track_config.get("effects", {})
            if effects_config:
                self.log_manager.log("verbose", f"    ‚ú® Applying Effects...")
                for effect_name, effect_params in effects_config.items():
                    if effect_name in self.effects:
                        target_track = effect_params.get("target_track", None)
                        effect_function = self.effects[effect_name]
                        params_for_effect = {k: v for k, v in effect_params.items() if k != "target_track"}

                        if target_track:
                            if target_track in track_data:
                                self.log_manager.log("verbose",
                                                    f"     Applying '{effect_name}' to {target_track} with params: {params_for_effect}")
                                try:
                                    track_data[target_track] = effect_function(track_data[target_track],
                                                                            **params_for_effect)
                                except Exception as e:
                                    self.log_manager.log("important",
                                                        f"‚ùå Error applying effect '{effect_name}' to {target_track}: {e}\n{traceback.format_exc()}")
                                    # Decide if error is fatal for the track: return False
                            else:
                                self.log_manager.log("important",
                                                    f"‚ö†Ô∏è Target track '{target_track}' for effect '{effect_name}' not found. Skipping effect.")
                        else:
                            self.log_manager.log("verbose",
                                                f"    Applying '{effect_name}' to all stems with params: {params_for_effect}")
                            for key in list(track_data.keys()):
                                try:
                                    track_data[key] = effect_function(track_data[key], **params_for_effect)
                                except Exception as e:
                                    self.log_manager.log("important",
                                                        f"‚ùå Error applying effect '{effect_name}' to {key}: {e}\n{traceback.format_exc()}")
                                    # Decide if error is fatal for the track: return False
                    else:
                        self.log_manager.log("important", f"‚ö†Ô∏è Effect '{effect_name}' not found. Skipping.")
            else:
                self.log_manager.log("verbose", "    ‚ú® No effects specified in JSON.")

            # Merge tracks (overlay method)
            self.log_manager.log("verbose", "    üîÑ Merging loaded stems...")
            merged_audio = None
            first_stem = True
            for stem_name in EXPECTED_STEMS:  # Use defined order
                if stem_name in track_data:
                    if first_stem:
                        merged_audio = track_data[stem_name]
                        first_stem = False
                        self.log_manager.log("verbose", f"     Base for merge: {stem_name}")
                    else:
                        try:
                            merged_audio = merged_audio.overlay(track_data[stem_name])
                            self.log_manager.log("verbose", f"     Overlayed: {stem_name}")
                        except Exception as e:
                            self.log_manager.log("important", f"‚ùå Error overlaying stem {stem_name}: {e}")
                            return False  # Treat overlay error as fatal for this mix

            # Export final MP3
            if merged_audio:
                self.log_manager.log("normal",
                                    f"    üíæ Exporting final MP3: {output_mp3_path} (Bitrate: {bitrate})")
                try:
                    os.makedirs(os.path.dirname(output_mp3_path), exist_ok=True)
                    merged_audio.export(output_mp3_path, format="mp3", bitrate=bitrate)
                    self.log_manager.log("important", f"‚úÖ Successfully exported: {output_mp3_name}")
                except Exception as e:
                    self.log_manager.log("important",
                                        f"‚ùå Error exporting MP3 {output_mp3_path}: {e}\n{traceback.format_exc()}")
                    return False  # Export failed
            else:
                self.log_manager.log("important",
                                    f"‚ö†Ô∏è No audio data was successfully merged for {output_mp3_name}. Skipping export.")
                return False  # Nothing to export is a failure condition

        self.log_manager.log("verbose", f"    üßπ Cleaned up temporary files for {source_zip_name}")
        return True  # Indicate success for this track

    def run(self):
        """Finds and processes all track JSON files."""
        self.log_manager.log("important", "=" * 40)
        self.log_manager.log("important", "üöÄ Starting MixTracks Processing")
        self.log_manager.log("important", f"Project Root: {self.project_root}")
        self.log_manager.log("important", f"Searching Folders: {self.look_folders}")
        self.log_manager.log("important", "=" * 40)

        track_json_files = self._find_track_json_files()

        if not track_json_files:
            self.log_manager.log("important", "‚èπÔ∏è No track JSON files found in the specified folders.")
            return

        self.log_manager.log("important", f"‚ÑπÔ∏è Found {len(track_json_files)} potential track JSON files to process.")

        success_count = 0
        error_count = 0  # Includes skips due to missing files, JSON errors, mix errors
        skipped_explicitly_count = 0  # Only for "ignore: true"

        for json_file in track_json_files:
            try:
                # --- Check skip based on export_parameters ---
                should_process = True
                try:
                    with open(json_file, 'r', encoding='utf-8') as f_check:
                        check_data = json.load(f_check)

                    # Check ignore flag first
                    mix_params = check_data.get("mix", {})
                    if mix_params.get("ignore", False) is True:
                        base_name = os.path.basename(json_file)
                        self.log_manager.log("normal", f"‚è≠Ô∏è Skipping {base_name} ('ignore': true in 'mix')")
                        skipped_explicitly_count += 1
                        should_process = False

                except Exception as json_read_error:
                    self.log_manager.log("important",
                                        f"‚ö†Ô∏è Could not pre-read JSON for skip checks {json_file}: {json_read_error}. Will attempt full processing.")
                    # Proceed to process_track which will handle the error properly

                if should_process:
                    if self.process_track(json_file):
                        success_count += 1
                    else:
                        error_count += 1  # Count errors/skips during processing

            except Exception as e:
                self.log_manager.log("important",
                                    f"üí• UNHANDLED CRITICAL ERROR during loop for {json_file}: {e}\n{traceback.format_exc()}")
                error_count += 1

        self.log_manager.log("important", "=" * 40)
        self.log_manager.log("important", "üèÅ Processing Complete")
        self.log_manager.log("important", f"üìä Summary:")
        self.log_manager.log("important", f"    ‚úÖ Successful Mixes: {success_count}")
        self.log_manager.log("important", f"    ‚è≠Ô∏è Skipped (ignore flags): {skipped_explicitly_count}")
        self.log_manager.log("important", f"    ‚ùå Errors/Failed Mixes: {error_count}")
        self.log_manager.log("important", "=" * 40)


// --- End File: ScriptUtils\Core\Udio\MixTracks.py ---

// --- Start File: ScriptUtils\Core\PromptContextCollector\PromptContextCollector.py ---

import os
import fnmatch
import re

class PromptContextCollector:
    def __init__(self, directories, includes, ignores, template_path, template_vars, output_path):
        self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..",".."))
        self.directories = directories
        self.includes = includes
        self.ignores = ignores
        self.template_path = os.path.join(self.project_root, template_path)
        self.output_path = os.path.join(self.project_root, output_path)  # ‚Üê Fix here
        self.template_vars = template_vars
        self.collected_files = []


    def _match_patterns(self, path, patterns):
        return any(fnmatch.fnmatch(path, pattern) for pattern in patterns)

    def _should_include(self, file_path):
        filename = os.path.basename(file_path)
        rel_path = os.path.relpath(file_path, self.project_root)
        return self._match_patterns(filename, self.includes) and not self._match_patterns(rel_path, self.ignores)

    def _resolve_paths(self):
        resolved = set()
        for pattern in self.directories:
            abs_pattern = os.path.join(self.project_root, pattern)
            if os.path.isdir(abs_pattern):
                resolved.add(abs_pattern)
            else:
                dir_part = os.path.dirname(abs_pattern) or "."
                for root, _, files in os.walk(dir_part):
                    for file in files:
                        full_path = os.path.join(root, file)
                        if fnmatch.fnmatch(full_path, abs_pattern):
                            resolved.add(os.path.dirname(full_path))
        return list(resolved)

    def _substitute_template(self):
        if not os.path.exists(self.template_path):
            raise FileNotFoundError(f"Template file not found: {self.template_path}")
        with open(self.template_path, "r", encoding="utf-8") as f:
            template = f.read()
        for key, value in self.template_vars.items():
            template = template.replace(f"{{{key}}}", str(value))
        return template

    def run(self):
        print(f"üõ† Starting PromptContextCollector")
        print(f"üìÅ Project Root: {self.project_root}")
        print(f"üìÇ Directories: {self.directories}")
        print(f"üìÑ Includes: {self.includes}")
        print(f"üö´ Ignores: {self.ignores}")
        print(f"üìú Template Path: {self.template_path}")
        print(f"üì§ Output Path: {self.output_path}")
        print("-" * 50)

        collected = 0
        with open(self.output_path, 'w', encoding='utf-8') as out:
            out.write("// --- Source Blob ---\n\n")

            for base_dir in self._resolve_paths():
                for root, _, files in os.walk(base_dir):
                    if self._match_patterns(os.path.relpath(root, self.project_root), self.ignores):
                        continue
                    for file in files:
                        file_path = os.path.join(root, file)
                        if not self._should_include(file_path):
                            continue
                        rel_path = os.path.relpath(file_path, self.project_root)
                        try:
                            with open(file_path, "r", encoding="utf-8") as src:
                                out.write(f"// --- Start File: {rel_path} ---\n\n")
                                out.write(src.read())
                                out.write(f"\n\n// --- End File: {rel_path} ---\n\n")
                            self.collected_files.append(rel_path)
                            print(f"‚úÖ Added: {rel_path}")
                            collected += 1
                        except Exception as e:
                            print(f"‚ùå Error reading {rel_path}: {e}")
                            out.write(f"// !!! Error reading file {rel_path}: {e} !!!\n\n")

            prompt_text = self._substitute_template()
            out.write("\n" + "-" * 50 + "\n")
            out.write("// --- Prompt ---\n\n")
            out.write(prompt_text)

        print("-" * 50)
        print(f"üéØ Collection complete: {collected} file(s) added.")
        print(f"üìù Output written to: {self.output_path}")


// --- End File: ScriptUtils\Core\PromptContextCollector\PromptContextCollector.py ---

// --- Start File: ScriptUtils\Core\ConfigManager.py ---

import os
import json

class ConfigManager:
    def __init__(self):
        base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))  # ScriptUtils/
        self.default_config_path = os.path.join(base_dir, "Config", "default_config.json")
        self.user_config_path = os.path.join(base_dir, "Config", "user_config.json")
        self.config = {}        
        self.load_config()
        
    def load_config(self):
        self.config = self.load_json(self.default_config_path)

        if os.path.exists(self.user_config_path):
            user_config = self.load_json(self.user_config_path)
            self.override_config(user_config)
        return self.config

    def load_json(self, path):
        if not os.path.exists(path):
            print(f"Warning: {path} not found.")
            return {}
        with open(path, 'r') as file:
            return json.load(file)

    def override_config(self, user_config):
        for key, value in user_config.items():            
            self.config[key] = value

 
            
    def print(self):
        print("\nFinal Merged Configuration:")
        print(json.dumps(self.config, indent=4))


// --- End File: ScriptUtils\Core\ConfigManager.py ---

// --- Start File: ScriptUtils\Core\LogManager.py ---

import os

# possible log levels: important/normal/verbose/disabled
class LogManager:
    def __init__(self, globalLogLevel):
        """
        Initializes the LogManager with a global log level.

        Args:
            globalLogLevel (str): The global log level ("important", "normal", "verbose", "disabled").
        """
        self.globalLogLevel = globalLogLevel.lower()
        self.logLevels = {
            "important": 3,
            "normal": 2,
            "verbose": 1,
            "disabled": 0,
        }
        if self.globalLogLevel not in self.logLevels:
            print(f"Error: Invalid global log level '{globalLogLevel}'.  Defaulting to 'disabled'.")
            self.globalLogLevel = "disabled"

    def log(self, message_log_level, message):
        """
        Logs a message if its level is at or above the global log level.

        Args:
            message_log_level (str): The log level of the message ("important", "normal", "verbose", "disabled").
            message (str): The message to log.
        """
        message_log_level = message_log_level.lower()
        if message_log_level not in self.logLevels:
            print(f"Error: Invalid message log level '{message_log_level}'.  Message not logged.")
            return  # Important: Exit if the level is invalid

        if self.logLevels.get(message_log_level, 0) >= self.logLevels.get(self.globalLogLevel, 0):
            print(message)

// --- End File: ScriptUtils\Core\LogManager.py ---

// --- Start File: ScriptUtils\Core\BackupUtil\BackupUtil.py ---

import os
import zipfile
import time
from datetime import datetime
from fnmatch import fnmatch
import sys

# Add Core to path so we can import ConfigManager, LogManager
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
from Core.ConfigManager import ConfigManager
from Core.LogManager import LogManager

class BackupUtil:
    def __init__(self, destination_path, folders, excludes=None):
        # Correcting the project root
        self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
        self.destination_path = destination_path
        self.folders = [os.path.join(self.project_root, folder) for folder in folders]  # Correct the folder paths
        self.excludes = excludes or []

        # Replace {DATE} in destination path
        current_date = datetime.now().strftime("%Y-%m-%d")
        self.destination_zip = os.path.abspath(destination_path.replace("{DATE}", current_date))
        self.compression_level = zipfile.ZIP_DEFLATED

        # Load config
        self.config_manager = ConfigManager()
        self.config = self.config_manager.load_config()
        self.log_level = self.config.get("log_level", "balanced")
        self.log_manager = LogManager(self.log_level) # Initialize LogManager

        self.log_manager.log("verbose", "=== BackupUtil Initialized ===")
        self.log_manager.log("verbose", "Project root: " + self.project_root)
        self.log_manager.log("verbose", "Folders to backup:")
        for folder in self.folders:
            self.log_manager.log("verbose", "  -> " + folder)
        self.log_manager.log("verbose", "Exclude patterns: " + str(self.excludes))
        self.log_manager.log("verbose", "Destination zip path: " + self.destination_zip)
        self.log_manager.log("verbose", "Compression level: ZIP_DEFLATED")

    def is_excluded(self, filepath):
        rel_path = os.path.relpath(filepath, self.project_root)
        result = any(fnmatch(rel_path, pattern) for pattern in self.excludes)
        if result:
            self.log_manager.log("verbose", "Excluded by pattern: " + rel_path)
        return result

    def get_next_filename(self, filename):
        base, ext = os.path.splitext(filename)
        counter = 2
        new_filename = f"{base} ({counter}){ext}"
        while os.path.exists(new_filename):
            counter += 1
            new_filename = f"{base} ({counter}){ext}"
        self.log_manager.log("normal", "Existing backup found. New name: " + new_filename)
        return new_filename

    def backup(self):
        start_time = time.time()
        self.log_manager.log("normal", "Starting backup...")

        # Check if the folders exist
        for folder in self.folders:
            if not os.path.exists(folder):
                self.log_manager.log("important", "Error: Folder not found - " + folder)

        if os.path.exists(self.destination_zip):
            self.log_manager.log("normal", self.destination_zip + " already exists.")
            self.destination_zip = self.get_next_filename(self.destination_zip)

        os.makedirs(os.path.dirname(self.destination_zip), exist_ok=True)
        self.log_manager.log("normal", "Creating archive: " + self.destination_zip)

        total_files = 0
        with zipfile.ZipFile(self.destination_zip, 'w', self.compression_level) as zipf:
            for folder in self.folders:
                if not os.path.exists(folder):
                    self.log_manager.log("important", "Warning: Folder not found - " + folder)
                    continue
                for root, dirs, files in os.walk(folder):
                    self.log_manager.log("verbose", "Scanning directory: " + root)
                    for file in files:
                        full_path = os.path.join(root, file)
                        if not os.path.exists(full_path):
                            self.log_manager.log("verbose", "Skipped non-existent: " + full_path)
                            continue
                        if self.is_excluded(full_path):
                            continue
                        arcname = os.path.relpath(full_path, self.project_root)
                        zipf.write(full_path, arcname)
                        total_files += 1
                        self.log_manager.log("verbose", "Added: " + arcname)

        duration = time.time() - start_time
        self.log_manager.log("normal", "Backup complete: " + self.destination_zip)
        self.log_manager.log("normal", "Files added: " + str(total_files))
        self.log_manager.log("normal", "Duration: " + str(duration) + " seconds")

    def run(self):
        self.backup()

// --- End File: ScriptUtils\Core\BackupUtil\BackupUtil.py ---

// --- Start File: ScriptUtils\Core\PromptContextCollector\PromptContextCollector.py ---

import os
import fnmatch
import re

class PromptContextCollector:
    def __init__(self, directories, includes, ignores, template_path, template_vars, output_path):
        self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..",".."))
        self.directories = directories
        self.includes = includes
        self.ignores = ignores
        self.template_path = os.path.join(self.project_root, template_path)
        self.output_path = os.path.join(self.project_root, output_path)  # ‚Üê Fix here
        self.template_vars = template_vars
        self.collected_files = []


    def _match_patterns(self, path, patterns):
        return any(fnmatch.fnmatch(path, pattern) for pattern in patterns)

    def _should_include(self, file_path):
        filename = os.path.basename(file_path)
        rel_path = os.path.relpath(file_path, self.project_root)
        return self._match_patterns(filename, self.includes) and not self._match_patterns(rel_path, self.ignores)

    def _resolve_paths(self):
        resolved = set()
        for pattern in self.directories:
            abs_pattern = os.path.join(self.project_root, pattern)
            if os.path.isdir(abs_pattern):
                resolved.add(abs_pattern)
            else:
                dir_part = os.path.dirname(abs_pattern) or "."
                for root, _, files in os.walk(dir_part):
                    for file in files:
                        full_path = os.path.join(root, file)
                        if fnmatch.fnmatch(full_path, abs_pattern):
                            resolved.add(os.path.dirname(full_path))
        return list(resolved)

    def _substitute_template(self):
        if not os.path.exists(self.template_path):
            raise FileNotFoundError(f"Template file not found: {self.template_path}")
        with open(self.template_path, "r", encoding="utf-8") as f:
            template = f.read()
        for key, value in self.template_vars.items():
            template = template.replace(f"{{{key}}}", str(value))
        return template

    def run(self):
        print(f"üõ† Starting PromptContextCollector")
        print(f"üìÅ Project Root: {self.project_root}")
        print(f"üìÇ Directories: {self.directories}")
        print(f"üìÑ Includes: {self.includes}")
        print(f"üö´ Ignores: {self.ignores}")
        print(f"üìú Template Path: {self.template_path}")
        print(f"üì§ Output Path: {self.output_path}")
        print("-" * 50)

        collected = 0
        with open(self.output_path, 'w', encoding='utf-8') as out:
            out.write("// --- Source Blob ---\n\n")

            for base_dir in self._resolve_paths():
                for root, _, files in os.walk(base_dir):
                    if self._match_patterns(os.path.relpath(root, self.project_root), self.ignores):
                        continue
                    for file in files:
                        file_path = os.path.join(root, file)
                        if not self._should_include(file_path):
                            continue
                        rel_path = os.path.relpath(file_path, self.project_root)
                        try:
                            with open(file_path, "r", encoding="utf-8") as src:
                                out.write(f"// --- Start File: {rel_path} ---\n\n")
                                out.write(src.read())
                                out.write(f"\n\n// --- End File: {rel_path} ---\n\n")
                            self.collected_files.append(rel_path)
                            print(f"‚úÖ Added: {rel_path}")
                            collected += 1
                        except Exception as e:
                            print(f"‚ùå Error reading {rel_path}: {e}")
                            out.write(f"// !!! Error reading file {rel_path}: {e} !!!\n\n")

            prompt_text = self._substitute_template()
            out.write("\n" + "-" * 50 + "\n")
            out.write("// --- Prompt ---\n\n")
            out.write(prompt_text)

        print("-" * 50)
        print(f"üéØ Collection complete: {collected} file(s) added.")
        print(f"üìù Output written to: {self.output_path}")


// --- End File: ScriptUtils\Core\PromptContextCollector\PromptContextCollector.py ---

// --- Start File: ScriptUtils\Core\Udio\AddNewTracks.py ---

import os
import shutil
import re
import sys
import json
from collections import defaultdict

# Add Core to path so we can import ConfigManager, LogManager
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
from Core.ConfigManager import ConfigManager
from Core.LogManager import LogManager

class AddNewTracks:
    """
    Moves or copies MP3 and ZIP file pairs into organized subfolders, handling naming and error conditions.
    All paths are relative to the UserProject root, which is calculated automatically.
    Also creates a default JSON file for each track.  JSON template is loaded in the run method.
    """
    def __init__(self, source_directory, track_target_path, move_files=True, log_level = None):
        """
        Initializes the AddNewTrack object.

        Args:
            source_directory (str): The path to the directory containing the MP3 and ZIP files,
                                     relative to the project root.
            track_target_path (str): The path to the target directory *relative* to the UserProject root
                                     where track subfolders will be created (e.g., "tracks").
            move_files (bool, optional): If True, files are moved; if False, files are copied. Defaults to True.
        """
        # Calculate project root dynamically
        self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
        self.source_path = os.path.join(self.project_root, source_directory)  # Make source_path absolute
        self.track_target_path = os.path.join(self.project_root, track_target_path)
        self.move_files = move_files
        self.files_processed_count = 0
        self.folders_created_count = 0        
        self.action_word = "Moved" if move_files else "Copied"
        self.file_operation = shutil.move if move_files else shutil.copy2
        self.count_description = "Files moved" if move_files else "Files copied"

        # Load config
        self.config_manager = ConfigManager()
        self.config = self.config_manager.load_config()
        self.log_manager = LogManager(self.config.get("log_level", "verbose")) # Initialize LogManager
        if log_level != None:
            self.log_manager.globalLogLevel = log_level


    def ensure_target_root_exists(self):
        """Ensures that the target root directory exists."""
        if not os.path.isdir(self.track_target_path):
            try:
                os.makedirs(self.track_target_path)
                self.log_manager.log("normal", f"üìÅ Created target root directory: {self.track_target_path}")
                self.folders_created_count += 1
            except OSError as e:
                self.log_manager.log("important", f"‚ùå Error: Could not create target root directory '{self.track_target_path}'. {e}")
                return False
        return True

    def get_source_files(self):
        """
        Gets all files from the source directory and performs initial validation.
        """
        try:
            all_items = os.listdir(self.source_path)
            source_files = [f for f in all_items if os.path.isfile(os.path.join(self.source_path, f))]
        except OSError as e:
            self.log_manager.log("important", f"‚ùå Error: Could not read source directory '{self.source_path}'. {e}")
            return None, None, None, None

        mp3_files = set()
        zip_files = set()
        basenames_mp3 = set()
        basenames_zip = set()

        for f in source_files:
            name, ext = os.path.splitext(f)
            ext_lower = ext.lower()
            if ext_lower == ".mp3":
                mp3_files.add(f)
                basenames_mp3.add(name)
            elif ext_lower == ".zip":
                zip_files.add(f)
                basenames_zip.add(name)
        return mp3_files, zip_files, basenames_mp3, basenames_zip

    def validate_file_pairs(self, basenames_mp3, basenames_zip):
        """Validates that there is a matching .mp3 and .zip file for each base name."""
        inconsistent_basenames = basenames_mp3.symmetric_difference(basenames_zip)

        if inconsistent_basenames:
            self.log_manager.log("important", "‚ùå Error: File inconsistency detected. Found '.mp3' without matching '.zip' or vice-versa.")
            self.log_manager.log("important", "Inconsistent base names:")
            for base in inconsistent_basenames:
                if base in basenames_mp3:
                    self.log_manager.log("important", f"  - Found '{base}.mp3' without matching '.zip'")
                else:
                    self.log_manager.log("important", f"  - Found '{base}.zip' without matching '.mp3'")
            return None

        return basenames_mp3.intersection(basenames_zip)

    def create_track_json(self, folder_name, track_name, json_template):
        """Creates the track JSON file."""
        if json_template is None:
            return

        # Construct the JSON file path.
        json_file_name = f"{track_name}.json"
        dest_json_path = os.path.join(self.track_target_path, folder_name, json_file_name)

        # Modify the template
        updated_template = json_template.copy()
        updated_template["mix"]["source_path"] = f"{track_name}.zip"
        updated_template["mix"]["output_file"] = f"{track_name}.mp3"

        try:
            with open(dest_json_path, 'w', encoding='utf-8') as f:
                json.dump(updated_template, f, indent=4)
            self.log_manager.log("normal", f"üìù Created JSON file: {os.path.join(folder_name, json_file_name)}")
        except Exception as e:
            self.log_manager.log("important", f"‚ùå Error: Could not create JSON file '{dest_json_path}': {e}")

    def process_files(self, valid_basenames, json_template):
        """Processes the valid MP3/ZIP file pairs."""
        if not valid_basenames:
            self.log_manager.log("important", "‚ÑπÔ∏è No MP3/ZIP pairs found in the source directory.")
            return

        self.log_manager.log("normal", f"‚ÑπÔ∏è Found {len(valid_basenames)} valid MP3/ZIP pairs for processing.")

        sorted_basenames = sorted(list(valid_basenames))

        for base_name_extless in sorted_basenames:
            source_mp3 = base_name_extless + ".mp3"
            source_zip = base_name_extless + ".zip"

            source_mp3_full_path = os.path.join(self.source_path, source_mp3)
            source_zip_full_path = os.path.join(self.source_path, source_zip)

            # Double check
            if not os.path.exists(source_mp3_full_path) or not os.path.exists(source_zip_full_path):
                self.log_manager.log("important", f"‚ö†Ô∏è Warning: Source file(s) for '{base_name_extless}' disappeared before processing. Skipping.")
                continue

            # Determine subfolder name
            match = re.match(r"^(.*?)(?: \(\d+\))?$", base_name_extless)
            folder_name = match.group(1).strip() if match else base_name_extless.strip()
            dest_folder_path = os.path.join(self.track_target_path, folder_name)
            relative_dest_folder = os.path.join(folder_name)

            # Create subfolder
            if not os.path.isdir(dest_folder_path):
                try:
                    os.makedirs(dest_folder_path)
                    self.log_manager.log("verbose", f"üìÅ Subfolder created: {relative_dest_folder}")
                    self.folders_created_count += 1
                except OSError as e:
                    self.log_manager.log("important", f"‚ùå Error: Could not create subfolder '{dest_folder_path}'. Skipping '{base_name_extless}'. {e}")
                    continue

            # Determine next number
            next_number = 1
            try:
                existing_files = os.listdir(dest_folder_path)
                max_num = 0
                pattern = re.compile(r"^" + re.escape(folder_name) + r" (\d+)\.(mp3|zip)$", re.IGNORECASE)
                for fname in existing_files:
                    file_match = pattern.match(fname)
                    if file_match:
                        num = int(file_match.group(1))
                        if num > max_num:
                            max_num = num
                next_number = max_num + 1
            except OSError as e:
                self.log_manager.log("important", f"‚ùå Error: Could not read destination subfolder '{dest_folder_path}'. Skipping '{base_name_extless}'. {e}")
                continue

            # Construct new paths
            new_mp3_name = f"{folder_name} {next_number}.mp3"
            new_zip_name = f"{folder_name} {next_number}.zip"
            dest_mp3_path = os.path.join(dest_folder_path, new_mp3_name)
            dest_zip_path = os.path.join(dest_folder_path, new_zip_name)

            # Process files
            try:
                # --- Process MP3 file ---
                self.file_operation(source_mp3_full_path, dest_mp3_path)
                log_dest_mp3 = os.path.join(relative_dest_folder, new_mp3_name)
                self.log_manager.log("normal", f"‚û°Ô∏è {self.action_word} {source_mp3} to {log_dest_mp3}")
                self.files_processed_count += 1

                # Create JSON file
                track_name = f"{folder_name} {next_number}"  # Base name for the track
                self.create_track_json(folder_name, track_name, json_template)


                try:
                    # --- Process ZIP file ---
                    if not os.path.exists(source_zip_full_path):
                        self.log_manager.log("important", f"‚ùå Error: Source ZIP '{source_zip}' disappeared before it could be {self.action_word.lower()}. MP3 was already {self.action_word.lower()}.")
                        if not self.move_files and os.path.exists(dest_mp3_path):
                            try:
                                os.remove(dest_mp3_path)
                                self.log_manager.log("important", f"üßπ Cleaned up partially {self.action_word.lower()} file: {log_dest_mp3}")
                                self.files_processed_count -= 1
                            except OSError as e_rem:
                                self.log_manager.log("important", f"‚ö†Ô∏è Warning: Could not clean up partially {self.action_word.lower()} MP3 '{log_dest_mp3}'. {e_rem}")
                    else:
                        self.file_operation(source_zip_full_path, dest_zip_path)
                        log_dest_zip = os.path.join(relative_dest_folder, new_zip_name)
                        self.log_manager.log("normal", f"‚û°Ô∏è {self.action_word} {source_zip} to {log_dest_zip}")
                        self.files_processed_count += 1

                except Exception as e_zip:
                    self.log_manager.log("important", f"‚ùå Error: Failed to {self.action_word.lower()} ZIP file '{source_zip}' after {self.action_word.lower()}ing MP3. {e_zip}")
                    if not self.move_files and os.path.exists(dest_mp3_path):
                        try:
                            os.remove(dest_mp3_path)
                            self.log_manager.log("important", f"üßπ Cleaned up partially {self.action_word.lower()} file: {log_dest_mp3}")
                            self.files_processed_count -= 1
                        except OSError as e_rem:
                            self.log_manager.log("important", f"‚ö†Ô∏è Warning: Could not clean up partially {self.action_word.lower()} MP3 '{log_dest_mp3}'. {e_rem}")

            except Exception as e_mp3:
                self.log_manager.log("important", f"‚ùå Error: Failed to {self.action_word.lower()} MP3 file '{source_mp3}'. Skipping pair. {e_mp3}")

    def run(self):
        """Runs the AddNewTrack process."""
        self.log_manager.log("normal", f"‚ÑπÔ∏è Source directory: {self.source_path}")
        self.log_manager.log("normal", f"‚ÑπÔ∏è Target root directory: {self.track_target_path}")
        self.log_manager.log("normal", f"‚ÑπÔ∏è Mode: {'Move files' if self.move_files else 'Copy files'}")
        self.log_manager.log("normal", "-" * 30)

        if not self.ensure_target_root_exists():
            sys.exit(1)

        # Load JSON template
        json_template_file = "AddNewTracksJsonTemplate.json"  # Default template name
        template_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), json_template_file) # Gets the directory of the current script
        if not os.path.exists(template_path):
            self.log_manager.log("important", f"‚ùå Error: JSON template file not found at '{template_path}'.")
            json_template = None  # Set to None to prevent errors later
        else:
            try:
                with open(template_path, 'r', encoding='utf-8') as f:
                    json_template = json.load(f)
            except json.JSONDecodeError as e:
                self.log_manager.log("important", f"‚ùå Error: Invalid JSON in template file '{template_path}': {e}")
                json_template = None

        mp3_files, zip_files, basenames_mp3, basenames_zip = self.get_source_files()
        if (mp3_files, zip_files, basenames_mp3, basenames_zip) == (None, None, None, None):
            sys.exit(1)

        valid_basenames = self.validate_file_pairs(basenames_mp3, basenames_zip)
        if valid_basenames is None:
            sys.exit(1)

        self.process_files(valid_basenames, json_template)

        self.log_manager.log("normal", "-" * 30)
        self.log_manager.log("normal", "‚úÖ Processing complete.")
        self.log_manager.log("normal", "üìä Summary:")
        self.log_manager.log("important", f"  ‚úÖ {self.count_description}: {self.files_processed_count}")
        self.log_manager.log("normal", f"  ‚úÖ Subfolders created: {self.folders_created_count}")

// --- End File: ScriptUtils\Core\Udio\AddNewTracks.py ---

// --- Start File: ScriptUtils\Core\Udio\AnalyzeTrackDB.py ---

import os
import sys
import io
import json
import hashlib
import time
from mutagen.mp3 import MP3

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

class AnalyzeTrackDB:
    def __init__(self, look_folders, estimate_bitrates):
        self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))  # project root
        self.look_folders = [os.path.join(self.project_root, folder) for folder in look_folders]
        self.estimate_bitrates = estimate_bitrates
        self.track_data = []

    def log(self, message):
        print(message)  # Output directly to console

    def get_mp3_duration(self, file_path):
        try:
            audio = MP3(file_path)
            duration = int(audio.info.length)
            minutes = duration // 60
            seconds = duration % 60
            return duration, f"{minutes}:{seconds:02d}"
        except Exception:
            return 0, "Unknown"

    def read_track_metadata_from_json(self, json_file):
        metadata = {"energy": None, "mood": None, "pop": None, "stars": None}
        if os.path.exists(json_file):
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    if "tags" in data:
                        metadata.update({
                            "energy": data["tags"].get("energy"),
                            "mood": data["tags"].get("mood"),
                            "pop": data["tags"].get("pop"),
                            "stars": data["tags"].get("stars")
                        })
            except Exception as e:
                self.log(f"‚ö†Ô∏è Error reading {json_file}: {e}")
        return metadata

    def estimate_size(self, total_duration):
        sizes = {}
        for bitrate in self.estimate_bitrates:
            kbps = int(bitrate.replace("k", ""))
            mb = (total_duration * kbps / 8) / 1024
            sizes[bitrate] = mb
        return sizes

    def run(self):
        start_time = time.time()

        total_size = 0
        total_duration = 0
        mp3_count = 0
        md5_hashes = {}
        name_duplicates = {}

        self.log("üîç Searching for MP3 files with corresponding JSON metadata...\n")

        for folder in self.look_folders:
            if not os.path.isdir(folder):
                self.log(f"‚ö†Ô∏è Folder not found: {folder}")
                continue

            self.log(f"üìÇ Searching in: {folder}")
            for root, _, files in os.walk(folder):
                for file in files:
                    if not file.lower().endswith(".mp3"):
                        continue

                    file_path = os.path.join(root, file)
                    json_file = os.path.splitext(file_path)[0] + ".json"
                    file_name = os.path.splitext(file)[0]
                    has_json = os.path.exists(json_file)
                    json_emoji = " üìÑ" if has_json else " üìÑ‚ùå"

                    if not has_json:
                        self.log(f"‚ö†Ô∏è Warning: JSON metadata not found for {file}")
                        continue

                    metadata = self.read_track_metadata_from_json(json_file)
                    energy = metadata["energy"]
                    mood = metadata["mood"]
                    pop = metadata["pop"]
                    stars = metadata["stars"]

                    duration_seconds, duration_str = self.get_mp3_duration(file_path)

                    md5 = hashlib.md5(open(file_path, 'rb').read()).hexdigest()
                    md5_hashes.setdefault(md5, []).append(file_path)
                    name_duplicates.setdefault(file, []).append(file_path)

                    # Format and log metadata
                    line = f"- {file} [{duration_str}]{json_emoji}"
                    if energy is not None: line += f" üî•{energy:.1f}"
                    if mood is not None:    line += f" üòä{mood:.1f}"
                    if pop is not None:     line += f" üéµ{pop:.1f}"
                    if stars is not None:   line += f" ‚ú®{int(stars)}"

                    self.log(line)

                    self.track_data.append({
                        "name": file_name,
                        "stars": int(stars) if stars is not None else None,
                        "mood": mood,
                        "energy": energy,
                        "pop": pop,
                    })

                    total_size += os.path.getsize(file_path)
                    total_duration += duration_seconds
                    mp3_count += 1

        self.log("\n=== Summary ===")
        self.log(f"üéº Total MP3 files analyzed (with JSON metadata): {mp3_count}")
        self.log(f"üíæ Total Space Occupied: {total_size / (1024 * 1024):.2f} MB")
        self.log(f"‚è±Ô∏è Total Duration: {total_duration // 3600:02d}:{(total_duration % 3600) // 60:02d}:{total_duration % 60:02d}")

        self.log("\nüìè Estimated Total Size at Different Bitrates:")
        for bitrate, size in self.estimate_size(total_duration).items():
            self.log(f"üì° {bitrate}: {size:.2f} MB")

        md5_duplicates = [v for v in md5_hashes.values() if len(v) > 1]
        name_duplicates_filtered = [v for v in name_duplicates.values() if len(v) > 1]

        if md5_duplicates or name_duplicates_filtered:
            self.log("\nüìÅ **Duplicate Files Found:**")
            count = 1
            for group in md5_duplicates:
                for i in range(len(group) - 1):
                    self.log(f"{count}. \"{group[i]}\" and \"{group[i+1]}\" (Same MD5)")
                    count += 1
            for group in name_duplicates_filtered:
                for i in range(len(group) - 1):
                    self.log(f"{count}. \"{group[i]}\" and \"{group[i+1]}\" (Same Name)")
                    count += 1

        elapsed = time.time() - start_time
        self.log(f"\nüïí Execution Time: {int(elapsed // 3600):02d}:{int((elapsed % 3600) // 60):02d}:{int(elapsed % 60):02d}")
        self.log("\n‚úÖ Finished analyzing files.")


// --- End File: ScriptUtils\Core\Udio\AnalyzeTrackDB.py ---

// --- Start File: ScriptUtils\Core\Udio\ExportTracks.py ---

import os
import shutil
import json
import sys

# Add Core to path so we can import ConfigManager, LogManager
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
from Core.ConfigManager import ConfigManager
from Core.LogManager import LogManager


class ExportTracks:
    def __init__(self, look_folders, unity_dest_path, global_log_level=None):
        """
        Initializes the ExportTracks class.

        Args:
            look_folders (list): A list of folders to search for track files.
            unity_dest_path (str): The destination path within the Unity project
                                    to copy the tracks to.
            global_log_level (str, optional): The global log level
                                    ("important", "normal", "verbose", "disabled").
                                    If None, it defaults to the value in the config file.
        """
        self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
        self.look_folders = [os.path.join(self.project_root, folder) for folder in look_folders]

        # Initialize ConfigManager
        self.config_manager = ConfigManager()
        self.config = self.config_manager.load_config()

        # Initialize LogManager
        self.log_manager = LogManager(self.config.get("log_level", "verbose"))
        if global_log_level is not None:
            self.log_manager.globalLogLevel = global_log_level

        self.unity_project_root = self.config.get("unity_project_root")
        if not self.unity_project_root:
            self.log_manager.log(
                "important",
                "‚ùå Unity project root is not set in the configuration.  Please set the 'unity_project_root' in config.json.",
            )
            sys.exit(1)  # Exit if the Unity project root is not configured
            
        self.unity_project_root = os.path.join(self.project_root, self.unity_project_root)            
        self.unity_dest_path = os.path.join(self.unity_project_root, unity_dest_path)
        os.makedirs(self.unity_dest_path, exist_ok=True)  # Ensure destination directory exists

    def _find_track_files(self):
        """Finds all track JSON files within the look_folders."""
        track_files = []
        for folder in self.look_folders:
            if not os.path.isdir(folder):
                self.log_manager.log("important", f"‚ùå Folder not found: {folder}")
                continue
            self.log_manager.log("verbose", f"üìÇ Searching for tracks in: {folder}")
            for root, _, files in os.walk(folder):
                for file in files:
                    if file.lower().endswith(".json"):
                        full_path = os.path.join(root, file)
                        track_files.append(full_path)
                        self.log_manager.log("verbose", f"    ‚úÖ Found track JSON: {full_path}")
        return track_files

    def _process_track(self, track_json_path):
        """
        Processes a single track JSON file, copying the MP3 and creating metadata.

        Args:
            track_json_path (str): Path to the track JSON file.
        """
        self.log_manager.log("normal", f"\n‚ñ∂Ô∏è Processing track: {track_json_path}")
        try:
            with open(track_json_path, "r", encoding="utf-8") as f:
                track_data = json.load(f)
        except FileNotFoundError:
            self.log_manager.log("important", f"‚ùå JSON file not found: {track_json_path}. Skipping.")
            return
        except json.JSONDecodeError as e:
            self.log_manager.log("important", f"‚ùå Error decoding JSON: {track_json_path} - {e}. Skipping.")
            return
        except Exception as e:
            self.log_manager.log("important", f"‚ùå Error reading JSON file: {track_json_path} - {e}. Skipping.")
            return

        # --- Check for 'mix' section and 'export' parameter ---
        if "mix" not in track_data:
            self.log_manager.log("important", f"‚ùå 'mix' section not found in JSON: {track_json_path}. Skipping.")
            return
        mix_config = track_data["mix"]

        export_params = track_data.get("export_parameters", {})
        if export_params.get("export", True) is False:  # Default to True if not present
            self.log_manager.log("normal", f"‚è≠Ô∏è Track not marked for export in JSON: {track_json_path}")
            return

        # --- Get MP3 source path ---
        source_mp3_name = mix_config.get("output_file")
        if not source_mp3_name:
            self.log_manager.log(
                "important", f"‚ùå 'output_file' not found in 'mix' section of JSON: {track_json_path}. Skipping."
            )
            return

        source_mp3_path = os.path.join(os.path.dirname(track_json_path), source_mp3_name)
        if not os.path.exists(source_mp3_path):
            self.log_manager.log("important", f"‚ùå MP3 file not found: {source_mp3_path}. Skipping.")
            return

        # --- Construct destination path and copy ---
        dest_mp3_path = os.path.join(self.unity_dest_path, source_mp3_name)
        try:
            if os.path.exists(dest_mp3_path):
                self.log_manager.log("normal", f"‚ö†Ô∏è  Overwriting existing file: {dest_mp3_path}")
            shutil.copy2(source_mp3_path, dest_mp3_path)  # copy2 preserves metadata
            self.log_manager.log("verbose", f"    ‚úÖ Copied MP3 to: {dest_mp3_path}")
        except Exception as e:
            self.log_manager.log("important", f"‚ùå Error copying MP3: {source_mp3_path} to {dest_mp3_path} - {e}")
            return

        # --- Create and save metadata JSON ---
        tags = track_data.get("tags", {})
        metadata = {"tags": tags}
        dest_meta_path = os.path.join(self.unity_dest_path, os.path.splitext(source_mp3_name)[0] + ".meta.json")
        try:
            with open(dest_meta_path, "w", encoding="utf-8") as f:
                json.dump(metadata, f, indent=4)
            self.log_manager.log("verbose", f"    ‚úÖ Created metadata: {dest_meta_path}")
        except Exception as e:
            self.log_manager.log("important", f"‚ùå Error writing metadata JSON: {dest_meta_path} - e")
            return

        self.log_manager.log("normal", f"    ‚úÖ Successfully processed track: {source_mp3_name}")

    def run(self):
        """Finds and processes all track JSON files."""
        self.log_manager.log("important", "=" * 40)
        self.log_manager.log("important", "üöÄ Starting Export Tracks to Unity")
        self.log_manager.log("important", f"Project Root: {self.project_root}")
        self.log_manager.log("important", f"Searching Folders: {self.look_folders}")
        self.log_manager.log("important", f"Exporting to: {self.unity_dest_path}")
        self.log_manager.log("important", "=" * 40)

        track_json_files = self._find_track_files()
        if not track_json_files:
            self.log_manager.log("important", "‚èπÔ∏è No track JSON files found.")
            return

        self.log_manager.log("normal", f"‚ÑπÔ∏è Found {len(track_json_files)} track JSON files to process.")
        for track_json_path in track_json_files:
            self._process_track(track_json_path)
        self.log_manager.log("important", "üèÅ Export process complete.")


// --- End File: ScriptUtils\Core\Udio\ExportTracks.py ---

// --- Start File: ScriptUtils\Core\Udio\MixTracks.py ---

import os
import zipfile
import json
import shutil
import importlib.util
import tempfile
import traceback
from pydub import AudioSegment
import sys
from fnmatch import fnmatch

# Add Core to path so we can import ConfigManager, LogManager
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
from Core.ConfigManager import ConfigManager
from Core.LogManager import LogManager

# --- Script Configuration ---
EXPECTED_STEMS = ["bass.wav", "drums.wav", "other.wav", "vocals.wav"]
DEFAULT_BITRATE = "192k"

class MixTracks:
    def __init__(self, look_folders, global_log_level=None, mix_override=None):
        """
        Initializes the MixTracks processor.

        Args:
            look_folders (list[str]): List of directory paths (relative to project root)
                                        to search for track subfolders.
            global_log_level (str, optional): Desired logging level ('verbose', 'normal', 'important').
                                            Defaults to config file setting or 'verbose'.
            mix_override (dict, optional):  A dictionary to override 'mix' section values in the JSON.
        """
        self.project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
        self.look_folders = [os.path.join(self.project_root, folder) for folder in look_folders]
        self.mix_override = mix_override  # Store the mix override

        # Initialize ConfigManager
        self.config_manager = ConfigManager()
        self.config = self.config_manager.load_config()

        # Initialize LogManager (using the provided snippet)
        self.log_manager = LogManager(self.config.get("log_level", "verbose"))
        if global_log_level != None:
            self.log_manager.globalLogLevel = global_log_level

        # Effects path, relative to the current script's directory
        self.effects_path = os.path.join(os.path.dirname(__file__), "Effects")
        self.effects = self._load_effects()

    def _find_track_json_files(self):
        """Finds all track JSON files (e.g., 'Track Name 1.json') within the look_folders."""
        json_files = []
        for folder in self.look_folders:
            if not os.path.isdir(folder):
                self.log_manager.log("important", f"‚ùå Search folder not found: {folder}")
                continue
            self.log_manager.log("verbose", f"üìÇ Searching for tracks in: {folder}")
            for root, _, files in os.walk(folder):
                for file in files:
                    # Process any .json file, assuming it's a track config
                    if file.lower().endswith(".json"):
                        full_path = os.path.join(root, file)
                        json_files.append(full_path)
                        self.log_manager.log("verbose", f"    ‚úÖ Mapped JSON: {full_path}")
        return json_files

    def _load_effects(self):
        """Loads effect functions from Python files in the Effects directory."""
        effects = {}
        if not os.path.isdir(self.effects_path):
            self.log_manager.log("important", f"‚ùå Effects directory not found: {self.effects_path}")
            return effects

        self.log_manager.log("normal", f"üîç Loading effects from: {self.effects_path}")
        for filename in os.listdir(self.effects_path):
            if filename.lower().endswith(".py") and filename != "__init__.py":
                effect_path = os.path.join(self.effects_path, filename)
                module_name = f"effects.{os.path.splitext(filename)[0]}"  # Unique module name
                try:
                    spec = importlib.util.spec_from_file_location(module_name, effect_path)
                    if spec and spec.loader:
                        module = importlib.util.module_from_spec(spec)
                        sys.modules[module_name] = module  # Add to sys.modules before exec
                        spec.loader.exec_module(module)

                        # Corrected Effect Loading Logic
                        if hasattr(module, "effect_name") and isinstance(module.effect_name, dict):
                            for name, function in module.effect_name.items():
                                if callable(function):
                                    if name in effects:
                                        self.log_manager.log("important",
                                                            f"‚ö†Ô∏è Duplicate effect name '{name}' found in {filename}. Overwriting previous.")
                                    effects[name] = function
                                    self.log_manager.log("verbose", f"    ‚úÖ Loaded effect: '{name}' from {filename}")
                                else:
                                    self.log_manager.log("important",
                                                        f"‚ö†Ô∏è Item '{name}' in 'effect_name' from {filename} is not callable.")
                        else:
                            self.log_manager.log("important",
                                                f"‚ö†Ô∏è Effect module {filename} missing 'effect_name' dictionary or it's not a dictionary.")
                    else:
                        self.log_manager.log("important", f"‚ö†Ô∏è Could not create module spec for {filename}")

                except Exception as e:
                    self.log_manager.log("important", f"‚ùå Error loading effect module {filename}: {e}\n{traceback.format_exc()}")
        self.log_manager.log("normal", f"‚ú® Loaded {len(effects)} effects: {', '.join(effects.keys())}")
        return effects

    def process_track(self, track_json_path):
        """
        Processes a single track based on its JSON configuration file.
        Returns True on success, False on failure/skip for this track.
        """
        self.log_manager.log("normal", f"\n‚ñ∂Ô∏è Processing track from JSON: {track_json_path}")
        config_file_dir = os.path.dirname(track_json_path)

        try:
            with open(track_json_path, 'r', encoding='utf-8') as f:
                config_data = json.load(f)
        except FileNotFoundError:
            self.log_manager.log("important", f"‚ùå JSON file not found: {track_json_path}")
            return False
        except json.JSONDecodeError as e:
            self.log_manager.log("important", f"‚ùå Error decoding JSON in: {track_json_path} - {e}")
            return False
        except Exception as e:
            self.log_manager.log("important", f"‚ùå Error reading JSON file {track_json_path}: {e}")
            return False

        # --- Get Mix Configuration ---
        track_config = config_data.get("mix")
        if not track_config or not isinstance(track_config, dict):
            self.log_manager.log("important",
                            f"‚ö†Ô∏è No 'mix' section found or it's not a dictionary in {track_json_path}. Skipping.")
            return False  # Treat as skippable failure

        # --- Apply Overrides ---
        if self.mix_override:
            # Check for target_wildcard
            target_wildcards = self.mix_override.get("target_wildcard", [])
            if target_wildcards:
                base_name = os.path.basename(track_json_path)
                for wildcard in target_wildcards:
                    if fnmatch(base_name, wildcard):
                        self.log_manager.log("verbose", f"   ‚úÖ Applying override for target_wildcard: {wildcard} on {base_name}")
                        track_config.update(self.mix_override.get("json", {}))  # Override the mix section
                        break  # Apply only once if matched
            else: # apply if no wildcards
                track_config.update(self.mix_override.get("json", {}))  # Override the mix section


        # --- Check Ignore Flag ---
        if track_config.get("ignore", False):
            output_name = track_config.get('output_file', os.path.basename(track_json_path).replace('.json', '.mp3'))
            self.log_manager.log("normal", f"‚è≠Ô∏è Skipping track {output_name} (ignore flag set in JSON)")
            return False  # Skipped, not a processing error

        # --- Get Paths and Parameters ---
        source_zip_name = track_config.get("source_path")
        output_mp3_name = track_config.get("output_file")
        bitrate = track_config.get("bitrate", DEFAULT_BITRATE)

        if not source_zip_name or not output_mp3_name:
            self.log_manager.log("important",
                            f"‚ùå Missing 'source_path' or 'output_file' in 'mix' section of {track_json_path}. Skipping.")
            return False

        source_zip_path = os.path.join(config_file_dir, source_zip_name)
        output_mp3_path = os.path.join(config_file_dir, output_mp3_name)

        if not os.path.exists(source_zip_path):
            self.log_manager.log("important", f"‚ùå Source ZIP file not found: {source_zip_path}")
            return False

        # --- Process Track ---
        with tempfile.TemporaryDirectory(prefix="mix_") as temp_folder:
            self.log_manager.log("verbose", f"    üì¶ Extracting {source_zip_name} to {temp_folder}")

            # Extract ZIP file
            try:
                with zipfile.ZipFile(source_zip_path, 'r') as zip_ref:
                    zip_ref.extractall(temp_folder)
            except zipfile.BadZipFile:
                self.log_manager.log("important", f"‚ùå Invalid ZIP file: {source_zip_path}")
                return False
            except Exception as e:
                self.log_manager.log("important", f"‚ùå Error extracting ZIP file {source_zip_path}: {e}")
                return False

            # Load expected audio stems and check for missing ones
            track_data = {}
            loaded_stems = []
            missing_stems = []
            for stem_file in EXPECTED_STEMS:
                file_path = os.path.join(temp_folder, stem_file)
                if os.path.exists(file_path):
                    try:
                        track_data[stem_file] = AudioSegment.from_wav(file_path)
                        loaded_stems.append(stem_file)
                        self.log_manager.log("verbose", f"    üîä Loaded stem: {stem_file}")
                    except Exception as e:
                        # Treat loading error as a missing stem for simplicity
                        self.log_manager.log("important",
                                            f"‚ùå Error loading stem {stem_file} from {source_zip_path}: {e}")
                        missing_stems.append(f"{stem_file} (Load Error)")
                else:
                    missing_stems.append(stem_file)

            # --- Strict Stem Check ---
            if missing_stems:
                self.log_manager.log("important",
                                    f"‚ùå Missing required stems in {source_zip_name}: {', '.join(missing_stems)}. Skipping mix.")
                return False  # Quit processing this track due to missing stems

            self.log_manager.log("verbose", f"    üîä All expected stems loaded: {', '.join(loaded_stems)}")

            # Apply effects dynamically
            effects_config = track_config.get("effects", {})
            if effects_config:
                self.log_manager.log("verbose", f"    ‚ú® Applying Effects...")
                for effect_name, effect_params in effects_config.items():
                    if effect_name in self.effects:
                        target_track = effect_params.get("target_track", None)
                        effect_function = self.effects[effect_name]
                        params_for_effect = {k: v for k, v in effect_params.items() if k != "target_track"}

                        if target_track:
                            if target_track in track_data:
                                self.log_manager.log("verbose",
                                                    f"     Applying '{effect_name}' to {target_track} with params: {params_for_effect}")
                                try:
                                    track_data[target_track] = effect_function(track_data[target_track],
                                                                            **params_for_effect)
                                except Exception as e:
                                    self.log_manager.log("important",
                                                        f"‚ùå Error applying effect '{effect_name}' to {target_track}: {e}\n{traceback.format_exc()}")
                                    # Decide if error is fatal for the track: return False
                            else:
                                self.log_manager.log("important",
                                                    f"‚ö†Ô∏è Target track '{target_track}' for effect '{effect_name}' not found. Skipping effect.")
                        else:
                            self.log_manager.log("verbose",
                                                f"    Applying '{effect_name}' to all stems with params: {params_for_effect}")
                            for key in list(track_data.keys()):
                                try:
                                    track_data[key] = effect_function(track_data[key], **params_for_effect)
                                except Exception as e:
                                    self.log_manager.log("important",
                                                        f"‚ùå Error applying effect '{effect_name}' to {key}: {e}\n{traceback.format_exc()}")
                                    # Decide if error is fatal for the track: return False
                    else:
                        self.log_manager.log("important", f"‚ö†Ô∏è Effect '{effect_name}' not found. Skipping.")
            else:
                self.log_manager.log("verbose", "    ‚ú® No effects specified in JSON.")

            # Merge tracks (overlay method)
            self.log_manager.log("verbose", "    üîÑ Merging loaded stems...")
            merged_audio = None
            first_stem = True
            for stem_name in EXPECTED_STEMS:  # Use defined order
                if stem_name in track_data:
                    if first_stem:
                        merged_audio = track_data[stem_name]
                        first_stem = False
                        self.log_manager.log("verbose", f"     Base for merge: {stem_name}")
                    else:
                        try:
                            merged_audio = merged_audio.overlay(track_data[stem_name])
                            self.log_manager.log("verbose", f"     Overlayed: {stem_name}")
                        except Exception as e:
                            self.log_manager.log("important", f"‚ùå Error overlaying stem {stem_name}: {e}")
                            return False  # Treat overlay error as fatal for this mix

            # Export final MP3
            if merged_audio:
                self.log_manager.log("normal",
                                    f"    üíæ Exporting final MP3: {output_mp3_path} (Bitrate: {bitrate})")
                try:
                    os.makedirs(os.path.dirname(output_mp3_path), exist_ok=True)
                    merged_audio.export(output_mp3_path, format="mp3", bitrate=bitrate)
                    self.log_manager.log("important", f"‚úÖ Successfully exported: {output_mp3_name}")
                except Exception as e:
                    self.log_manager.log("important",
                                        f"‚ùå Error exporting MP3 {output_mp3_path}: {e}\n{traceback.format_exc()}")
                    return False  # Export failed
            else:
                self.log_manager.log("important",
                                    f"‚ö†Ô∏è No audio data was successfully merged for {output_mp3_name}. Skipping export.")
                return False  # Nothing to export is a failure condition

        self.log_manager.log("verbose", f"    üßπ Cleaned up temporary files for {source_zip_name}")
        return True  # Indicate success for this track

    def run(self):
        """Finds and processes all track JSON files."""
        self.log_manager.log("important", "=" * 40)
        self.log_manager.log("important", "üöÄ Starting MixTracks Processing")
        self.log_manager.log("important", f"Project Root: {self.project_root}")
        self.log_manager.log("important", f"Searching Folders: {self.look_folders}")
        self.log_manager.log("important", "=" * 40)

        track_json_files = self._find_track_json_files()

        if not track_json_files:
            self.log_manager.log("important", "‚èπÔ∏è No track JSON files found in the specified folders.")
            return

        self.log_manager.log("important", f"‚ÑπÔ∏è Found {len(track_json_files)} potential track JSON files to process.")

        success_count = 0
        error_count = 0  # Includes skips due to missing files, JSON errors, mix errors
        skipped_explicitly_count = 0  # Only for "ignore: true"

        for json_file in track_json_files:
            try:
                # --- Check skip based on export_parameters ---
                should_process = True
                try:
                    with open(json_file, 'r', encoding='utf-8') as f_check:
                        check_data = json.load(f_check)

                    # Check ignore flag first
                    mix_params = check_data.get("mix", {})
                    if mix_params.get("ignore", False) is True:
                        base_name = os.path.basename(json_file)
                        self.log_manager.log("normal", f"‚è≠Ô∏è Skipping {base_name} ('ignore': true in 'mix')")
                        skipped_explicitly_count += 1
                        should_process = False

                except Exception as json_read_error:
                    self.log_manager.log("important",
                                        f"‚ö†Ô∏è Could not pre-read JSON for skip checks {json_file}: {json_read_error}. Will attempt full processing.")
                    # Proceed to process_track which will handle the error properly

                if should_process:
                    if self.process_track(json_file):
                        success_count += 1
                    else:
                        error_count += 1  # Count errors/skips during processing

            except Exception as e:
                self.log_manager.log("important",
                                    f"üí• UNHANDLED CRITICAL ERROR during loop for {json_file}: {e}\n{traceback.format_exc()}")
                error_count += 1

        self.log_manager.log("important", "=" * 40)
        self.log_manager.log("important", "üèÅ Processing Complete")
        self.log_manager.log("important", f"üìä Summary:")
        self.log_manager.log("important", f"    ‚úÖ Successful Mixes: {success_count}")
        self.log_manager.log("important", f"    ‚è≠Ô∏è Skipped (ignore flags): {skipped_explicitly_count}")
        self.log_manager.log("important", f"    ‚ùå Errors/Failed Mixes: {error_count}")
        self.log_manager.log("important", "=" * 40)


// --- End File: ScriptUtils\Core\Udio\MixTracks.py ---


--------------------------------------------------
// --- Prompt ---

ScriptUtils package is helper library that contains the scripts used across the Unity projects. These are primarily Python and Windows batch (.cmd/.bat) files designed to automate and streamline development workflows.

## üìÅ Structure

```
UserProject/ 
‚îú‚îÄUserScripts/    
‚îî‚îÄScriptUtils/ 
  ‚îú‚îÄ‚îÄ Core/       
  ‚îú‚îÄ‚îÄ UserScriptsExamples/    
  ‚îî‚îÄ‚îÄ Config/
```

## üìÇ Folder Descriptions

*Core/* 
Contains the core logic and implementation of helper utilities. These modules are not intended to be executed directly but are used as building blocks for scripts in the Scripts/ folder.

*UserScripts/*
Contains user-executable scripts. These typically serve as entry points and are composed by combining and calling logic from the Core/ directory. Each script solves a specific task like project setup, builds, cleaning temporary files, importing assets to Unity, etc. This folder is under source control of root project.

*UserScriptsExamples/*
Contains user-executable scripts examples.

*Config/*
Stores user- or machine-specific settings and environment variables. Useful for maintaining local configuration without affecting shared scripts.

## Folder Usage Recap
‚úÖ UserScripts - Real, production-ready entry point scripts (under source control)
üìå All paths in scripts should be interpreted relative to: UserProject folder (example: d:\projects\dev\UserProject). This is considred as a root folder of the project.


- Rewrite provided git_submodule_manager script as a module for ScriptUtils ( Core component )
- Give also example of using git_submodule_manager as a user script
- Use provided LogManager, self.log_manager.log("important", "message"). 4 log levels logLevels = {
            "important": 3,
            "normal": 2,
            "verbose": 1,
            "disabled": 0,
- pass parameters in init 
  - support overriding global_log_level (as in examples)